---
description: 'Author: 윤건우'
---

# 1. Vision Transformers

&#x20;   2017년 자연어 처리 분야에서 트랜스포머(transformer)의 등장 이후 이를 기반으로 분류, 클러스터링, 검색, 생성 모델링과 같은 다양한 언어 작업에서 성공을 이루었습니다.&#x20;

&#x20;   이 성공을 컴퓨터 비전 분야로 확장하여 <mark style="background-color:yellow;">2020년 Vision Transformer(ViT)</mark>를 개발했습니다. ViT는 이전의 표준이었던 합성곱 신경망(CNN)보다 **이미지 인식 작업**에서 뛰어난 성능을 보입니다. 원래의 트랜스포머처럼 **ViT는 이미지와 같은 비정형 데이터 (unstructured data)를** 다양한 작업(e.g., 분류)에 사용할 수 있는 표현으로 **변환(transform)합니다**.

<details>

<summary>이미지는 격자(grid) 형태의 정형 데이터 (structured data)가 아닌가요?</summary>

**이미지는** 픽셀 단위로 정규 격자 구조 (regular grid structure)를 가지고 있지만,  **데이터 분석과 머신러닝에서 비정형 데이터로 간주**됩니다. 이러한 분류가 이루어지는 이유는 <mark style="background-color:yellow;">이미지에 담긴 정보와 패턴이 전통적인 데이터 분석 도구로 쉽게 해석할 수 있도록 명시적으로 조직되어 있지 않기 (not explicitly organized)</mark> 때문입니다. 그 이유는 다음과 같습니다:

1. **의미적 콘텐츠 (Semantic Content)**: 이미지에 포함된 객체, 장면, 행동 등의 의미 있는 콘텐츠는 명시적으로 구조화되어 있지 않습니다 (not explicitly labeled or structured). 이를 추출하기 위해서는 고급 처리가 필요합니다.
2. **복잡성 (Complexity)**: 이미지 내의 패턴과 특징은 매우 복잡하고 다양할 수 있어 단순한 구조적 데이터 모델로 표현하기 어렵습니다.
3. **데이터 표현 (Data Representation)**: 픽셀 격자 (pixel grid)는 구조화되어 있지만, **픽셀 값 자체는** 이미지의 고수준 **정보나** 다양한 부분 간의 **관계를 직접적으로 전달하지 않습니다**.

</details>

\[그림 입력]: Both the original transformer as well as the ViT take unstructured data, convert it to numerical representations, and finally use that for tasks like classification.



&#x20;   트랜스포머와 ViT는 인코더 (encoder)로써 <mark style="background-color:yellow;">입력을 숫자 표현 (numerical representations)으로 변환</mark>합니다. 인코더가 작업을 수행하기 전에 두 데이터에 대한 전처리 (preprocess)가 필요합니다. **텍스트 입력** (textual input)은 **먼저 토큰화** (tokenized)되어야 합니다.



\[그림 입력]: Text is passed to one or multiple encoders by first tokenizing it using a tokenizer.



&#x20;   이미지는 단어로 구성되지 않기 때문에 일반적인 토큰화 과정을 사용할 수 없습니다. 대신, ViT의 저자들은 <mark style="background-color:yellow;">이미지를 "단어 (words)"로 토큰화하는 방법</mark>을 개발하여 **원래의 인코더 구조를 사용할 수 있게 했습니다**.

&#x20;   ViT는 텍스트 토큰 대신 **이미지를 패치로 분할**합니다. 예를 들어, 512x512 픽셀의 이미지를 작은 패치로 나눕니다. 개별 픽셀은 많은 정보를 전달하지 않지만, <mark style="background-color:yellow;">픽셀 패치를 결합하면 점점 더 많은 정보를 볼 수 있습니다</mark>. 이를 통해 모델이 패치에서 더 많은 정보를 추출할 수 있게 합니다.



\[그림 입력]: The “tokenization” process for image input. It converts an image into patches of sub-images.



&#x20;   **텍스트를 토큰화**하는 것처럼 **이미지는 패치로** 나뉩니다. 이러한 패치는 선형 임베딩되어 (linearly embedded) 수치적 표현 (_i.e._,임베딩; embeddings)을 생성하여 트랜스포머 모델의 입력으로 사용됩니다. 텍스트 토큰과 달리, **이미지 패치는** 각 이미지 별로 고유하여 **간단히 토큰 ID를 할당할 수 없습니다**.

<details>

<summary>텍스트와 이미지에 대한 토큰 ID 할당</summary>

텍스트는 단어, 부분 단어, 또는 문자 단위로 나누어 토큰화 되어 각 토큰에 고유한 ID를 할당 할 수 있습니다.

* 예를 들어, "Hello, world!"라는 문장은 \["Hello", ",", "world", "!"]와 같이 토큰화될 수 있습니다.
* 이 다음, 각 토큰에 고유한 ID를 부여합니다. 이 ID는 보통 모델의 어휘 사전(vocabulary)에 정의되어 있습니다. 예를 들어, "Hello"는 123, ","는 456, "world"는 789와 같은 ID를 가질 수 있습니다.



</details>



