---
description: 'Author: 윤건우'
---

# 2. Multimodal Embedding

임베딩은 대규모 정보를 효율적으로 캡처하고 방대한 데이터 내에서 정확한 검색을 돕습니다. 이번 장에서는 텍스트(textual)와 시각적 표현(vision representations)을 모두 캡처할 수 있는 임베딩 모델에 대해 살펴봅니다.&#x20;

그림 입력: Multimodal embedding models can create embeddings for multiple modalities in the same vector space.



멀티모달 임베딩의 **큰 장점**은 <mark style="background-color:yellow;">이종의 데이터를 동일한 벡터 공간(the same vector space)에 위치시켜 비교할 수 있다는 점</mark>입니다. 예를 들어, "강아지 사진" 같은 텍스트를 기반으로 이미지를 찾거나, 이미지와 관련된 문서를 찾을 수 있습니다.

그림 입력: Multimodal embedding models can create embeddings for multiple modalities in the same vector space.



다양한 멀티모달 임베딩 방법이 있지만, 현재 가장 유명하고 널리 사용되는 멀티모달 임베딩 모델은 CLIP(Contrastive Language-Image Pre-Training)입니다.

## Reference&#x20;

1. [Hands on Large Language Models](https://learning.oreilly.com/library/view/hands-on-large-language/9781098150952/ch05.html#transformers\_for\_vision), O'REILLY, 2024.
2. Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., & others (2021). Learning transferable visual models from natural language supervision. In _International conference on machine learning_ (pp. 8748–8763).
